### **Cornell Notes: Research Proposal Analysis**

**Topic:** Assessing Faithfulness and Utility of LLM-Generated Explanations for Black-Box Clinical Prediction Models
**Date:** October 26, 2023

| Cues & Keywords | Main Notes |
| :--- | :--- |
| **Project Goal & Title** | **1. Title & Abstract**<br>• **Title:** *Assessing Faithfulness and Utility of LLM-Generated Explanations for Black-Box Clinical Prediction Models.*<br>• **Abstract Summary:** This is a quantitative and qualitative study comparing explanations from Large Language Models (LLMs, specifically GPT) with a standard method (SHAP) for a diabetes risk prediction model. <br>• **Core Aims:**<br>  &nbsp;&nbsp;&nbsp;&nbsp;1. Formulate and test specific hypotheses on **faithfulness**, **comprehensibility**, and **clinical utility**.<br>  &nbsp;&nbsp;&nbsp;&nbsp;2. Use a controlled user study with statistical validation.<br>• **Key Deliverables:**<br>  &nbsp;&nbsp;&nbsp;&nbsp;1. A reproducible evaluation framework.<br>  &nbsp;&nbsp;&nbsp;&nbsp;2. New metrics that combine faithfulness and utility.<br>  &nbsp;&nbsp;&nbsp;&nbsp;3. Guidelines for using LLM explanations in clinical settings. |
| **Problem & Motivation** | **2. Introduction & Motivation**<br>• **Context:** "Black-box" AI models in healthcare are powerful but their lack of transparency creates risks of misdiagnosis and legal issues.<br>• **Problem:** LLMs can provide natural, human-like explanations, but it's unknown if these explanations are **faithful** (truly reflecting the model's internal logic) or **clinically useful** (actually helping doctors make better decisions).<br>• **Goal:** To rigorously test if LLM explanations can reliably replace or enhance existing, more technical explanation methods like SHAP in a real-world clinical workflow. |
| **What is being tested?** | **3. Research Questions (RQs) & Hypotheses (Hs)**<br>• **RQ1 (Faithfulness):** Do LLM explanations identify the same important features as SHAP?<br>  &nbsp;&nbsp;&nbsp;&nbsp;• **H1:** There will be a ≥80% feature overlap (measured by Jaccard similarity) between features mentioned by the LLM and the top-5 features identified by SHAP.<br>• **RQ2 (Comprehensibility):** Are LLM explanations easier for clinicians to understand?<br>  &nbsp;&nbsp;&nbsp;&nbsp;• **H2:** The mean comprehension rating (1-7 Likert scale) for LLM explanations will be at least 1 point higher than for SHAP explanations (tested with a paired t-test, α=0.05).<br>• **RQ3 (Clinical Utility):** Do LLM explanations lead to better clinical decisions?<br>  &nbsp;&nbsp;&nbsp;&nbsp;• **H3:** Clinicians using LLM explanations will achieve at least a 5% higher diagnostic accuracy on test cases compared to when using SHAP (tested with McNemar's test). |
| **Research Gap** | **4. Literature Review & Gap Analysis**<br>• **Existing XAI:** Methods like SHAP, LIME, and Integrated Gradients exist. They are quantitative and grounded in the model but are often terse and not intuitive for non-experts.<br>• **LLM-XAI Research:** Recent surveys (e.g., Bilal et al., 2025) call for more domain-specific, quantitative studies to validate LLM explanations.<br>• **Identified Gap:** **No existing studies have quantitatively measured the impact of LLM-generated explanations on actual clinical decision-making.** |
| **How will it be done? (Methodology)** | **5. Methodology**<br>**5.1 Data and Domain**<br>• **Dataset:** UCI Pima Indians Diabetes dataset. Contains 768 patient records with features like age, BMI, glucose, blood pressure, etc.<br>• **Preprocessing:** Standard data scaling and k-Nearest Neighbors (k-NN) imputation for missing values.<br><br>**5.2 Model Architectures**<br>• **Prediction Models:** Random Forest and XGBoost (common "black-box" models).<br>• **Baseline Explanations:** SHAP (TreeExplainer), a gold standard for feature attribution.<br>• **Proposed Explanations:** GPT-4, prompted to generate feature-based narrative explanations.<br><br>**5.3 Explanation Generation**<br>• **Prompting:** A structured template is used: `“Model prognosis: Diabetes=Yes for patient with features {...}. In plain language, explain why.”`<br>• **Quality Control:** For each case, 3 explanation variants are generated. The one with the highest log-probability (a measure of coherence) is selected.<br><br>**5.4 Evaluation Framework**<br>• **Faithfulness:** Features are extracted from the LLM's text using NLP. Jaccard similarity is calculated against the top-5 SHAP features for the same case.<br>• **Comprehensibility:** A user study with 10 clinicians who will each rate 20 explanations (a mix of LLM and SHAP) on a 1-7 scale for clarity, usefulness, and conciseness.<br>• **Clinical Utility:** Clinicians are given a patient's model score and an explanation, then must decide to "treat" or "monitor." Their accuracy and confidence are measured.<br><br>**5.5 Statistical Analysis**<br>• **Feature Overlap (H1):** One-sample t-test (to see if the mean Jaccard similarity is ≥ 0.8).<br>• **Comprehension (H2):** Paired t-test (to compare Likert scores for LLM vs. SHAP).<br>• **Decision Accuracy (H3):** McNemar's test (to compare paired proportions of correct/incorrect decisions). |
| **Study Design** | **6. Experimental Design**<br>• **Within-subjects:** Each clinician evaluates both LLM and SHAP explanations, but for different (though matched) patient cases. This controls for individual clinician skill/bias.<br>• **Ablation Studies:** The study will also test how results change when varying the LLM prompt detail, LLM temperature (randomness), and the number of SHAP features shown. |
| **Ethics & Safety** | **7. Ethical Considerations & IRB**<br>• **Human Subjects:** A formal Institutional Review Board (IRB) approval is required for the clinician study.<br>• **Data Privacy:** All patient data will be de-identified or synthetic to protect privacy.<br>• **Bias Mitigation:** The demographic balance of both the dataset and the recruited clinician pool will be checked and reported. |
| **Expected Impact** | **8. Expected Contributions & Novelty**<br>1. **Framework:** A complete, reproducible pipeline for evaluating clinical AI explanations.<br>2. **Metrics:** A novel, combined "faithfulness-utility score" to holistically assess explanations.<br>3. **Guidelines:** Evidence-based best practices for prompt engineering in clinical XAI. |
| **Timeline & Budget** | **9. Work Plan & Timeline (10 Weeks)**<br>• **Wks 1-2:** Data prep, model training.<br>• **Wks 3-4:** Implement SHAP & LLM explanation generation.<br>• **Wks 5-6:** Design the clinician study interface and protocol.<br>• **Wks 7-8:** Recruit participants, run the user study, collect data.<br>• **Wks 9-10:** Analyze results, write the final paper.<br><br>**10. Resources & Budget (~$1,000)**<br>• **Compute (GPU):** ~$300<br>• **LLM API Costs:** ~$200<br>• **Participant Compensation:** ~$500 (10 clinicians x $50) |
| **Risks & Solutions** | **11. Risks & Mitigations**<br>• **Low Participation:** Mitigate by recruiting through professional networks and increasing compensation if needed.<br>• **LLM Hallucinations:** Mitigate by using prompt validation techniques and performing manual spot-checks on generated explanations.<br>• **Data Imbalance:** Mitigate by re-sampling the data or using augmentation techniques for minority classes to ensure the model isn't biased. |
| **Citations** | **12. References**<br>• Key papers cited are foundational works in XAI (Lundberg & Lee on SHAP, Ribeiro et al. on LIME) and a forward-looking survey on LLMs for XAI (Bilal et al.). |
---
**Summary**

This research proposal details a rigorous, 10-week study to evaluate if LLM-generated explanations for a "black-box" diabetes prediction model are superior to the technical SHAP method. The study will assess three key dimensions: **faithfulness** (do they reflect the model's logic?), **comprehensibility** (are they easy for doctors to understand?), and **clinical utility** (do they improve diagnostic accuracy?). Through a controlled user study with 10 clinicians, the project will test specific, falsifiable hypotheses using statistical methods. The key novelty lies in being one of the first studies to measure the direct impact of LLM explanations on clinical decision-making, with the goal of producing a reusable evaluation framework and practical guidelines for the safe use of AI in healthcare. The project is well-scoped with a clear timeline, a modest budget of ~$1,000, and identified risks with mitigation strategies.

# Refs:
- https://notebooklm.google.com/notebook/80586287-e2e7-491a-bbca-f99a231b621f (notebooklm)