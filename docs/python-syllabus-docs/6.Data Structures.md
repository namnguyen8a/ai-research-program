## Python Data Structures and Algorithms (DSA) Reference

This document provides a comprehensive guide to Data Structures and Algorithms in Python. It covers fundamental concepts, implementations, complexity analysis, and typical use-cases. Intended as a study reference and implementation guide for interview preparation, coursework, and practical projects.

---

### Table of Contents

1. [Introduction](#introduction)
2. [Complexity Analysis](#complexity-analysis)
3. [Built-in Data Structures](#built-in-data-structures)

   * [Lists](#lists)
   * [Tuples](#tuples)
   * [Sets](#sets)
   * [Dictionaries](#dictionaries)
4. [Custom Data Structures](#custom-data-structures)

   * [Arrays (Using Lists)](#arrays-using-lists)
   * [Linked Lists](#linked-lists)

     * [Singly Linked List](#singly-linked-list)
     * [Doubly Linked List](#doubly-linked-list)
     * [Circular Linked List](#circular-linked-list)
   * [Stacks](#stacks)
   * [Queues](#queues)

     * [Simple Queue](#simple-queue)
     * [Deque (Double-Ended Queue)](#deque-double-ended-queue)
   * [Hash Table (Hash Map)](#hash-table-hash-map)
   * [Trees](#trees)

     * [Binary Tree](#binary-tree)
     * [Binary Search Tree (BST)](#binary-search-tree-bst)
     * [Heap (Priority Queue)](#heap-priority-queue)
     * [Trie (Prefix Tree)](#trie-prefix-tree)
   * [Graphs](#graphs)
   * [Advanced Structures](#advanced-structures)

     * [Union-Find (Disjoint Set)](#union-find-disjoint-set)
     * [Segment Tree](#segment-tree)
     * [Fenwick Tree (Binary Indexed Tree)](#fenwick-tree-binary-indexed-tree)
5. [Algorithms](#algorithms)

   * [Searching](#searching)

     * [Linear Search](#linear-search)
     * [Binary Search](#binary-search)
   * [Sorting](#sorting)

     * [Bubble Sort](#bubble-sort)
     * [Selection Sort](#selection-sort)
     * [Insertion Sort](#insertion-sort)
     * [Merge Sort](#merge-sort)
     * [Quick Sort](#quick-sort)
     * [Heap Sort](#heap-sort)
   * [Recursion](#recursion)
   * [Dynamic Programming](#dynamic-programming)

     * [Memoization](#memoization)
     * [Tabulation](#tabulation)
   * [Greedy Algorithms](#greedy-algorithms)
   * [Backtracking](#backtracking)
   * [Graph Algorithms](#graph-algorithms)

     * [Traversal: BFS and DFS](#traversal-bfs-and-dfs)
     * [Shortest Path: Dijkstra, Bellman-Ford](#shortest-path-dijkstra-bellman-ford)
     * [Minimum Spanning Tree: Kruskal, Prim](#minimum-spanning-tree-kruskal-prim)
     * [Topological Sort](#topological-sort)
   * [Divide and Conquer](#divide-and-conquer)
6. [Problem-Solving Patterns](#problem-solving-patterns)

   * [Sliding Window](#sliding-window)
   * [Two Pointers](#two-pointers)
   * [Fast and Slow Pointers](#fast-and-slow-pointers)
   * [Binary Search on Answer](#binary-search-on-answer)
   * [Bit Manipulation](#bit-manipulation)
   * [Windowed Data Structures](#windowed-data-structures)
7. [Testing and Debugging](#testing-and-debugging)
8. [Tips for Interviews](#tips-for-interviews)
9. [Conclusion](#conclusion)

---

## Introduction

Data Structures organize data for efficient access, modification, and storage. Algorithms define step-by-step procedures to solve problems. Mastery of DSA in Python involves understanding built-ins, implementing custom structures, analyzing time/space complexities, and applying algorithmic patterns to real problems.

---

## Complexity Analysis

* **Time Complexity (Big O)**: Upper bound on time as input size grows.

  * Common notations: O(1), O(log n), O(n), O(n log n), O(n^2), etc.
* **Space Complexity**: Memory usage relative to input size.
* **Amortized Analysis**: Average time per operation over a sequence (e.g., dynamic array append is amortized O(1)).
* **Best, Average, Worst Cases**: Understand scenarios.

When implementing or selecting algorithms, always annotate expected complexities in comments or documentation.

---

## Built-in Data Structures

Python provides high-level data structures optimized in C.

### Lists

* Dynamic arrays: random access O(1), append amortized O(1), insert/delete at arbitrary index O(n).
* Methods: `append`, `pop`, `insert`, `remove`, `sort`, slicing, comprehension.

```python
lst = [1, 2, 3]
lst.append(4)  # [1,2,3,4]
lst.pop()      # 4
lst.insert(1, 10)  # [1,10,2,3]
```

### Tuples

* Immutable ordered sequences: fixed size, can be used as dictionary keys.

```python
tup = (1, 2, 3)
# tup[0] = 5  # TypeError
```

### Sets

* Unordered collections of unique elements. Average O(1) insertion, deletion, membership test.
* Methods: `add`, `remove`, `discard`, `pop`, set operations (union, intersection).

```python
s = {1, 2, 3}
s.add(4)
print(2 in s)  # True
```

### Dictionaries

* Hash tables mapping keys to values. Average O(1) lookup, insertion, deletion.
* Methods: `get`, `keys`, `values`, `items`, `pop`, comprehension.

```python
d = {"a": 1, "b": 2}
print(d.get("c", 0))  # 0
d["c"] = 3
```

---

## Custom Data Structures

### Arrays (Using Lists)

* Fixed-size arrays not native in Python, but lists serve as dynamic arrays.
* For fixed-size use-cases, consider `array` module or `collections.deque` for efficient pops from both ends.

### Linked Lists

#### Singly Linked List

```python
class Node:
    def __init__(self, val):
        self.val = val
        self.next = None

class SinglyLinkedList:
    def __init__(self):
        self.head = None

    def insert_at_head(self, val):
        node = Node(val)
        node.next = self.head
        self.head = node

    def insert_at_tail(self, val):
        node = Node(val)
        if not self.head:
            self.head = node
            return
        curr = self.head
        while curr.next:
            curr = curr.next
        curr.next = node

    def delete(self, val):
        dummy = Node(0)
        dummy.next = self.head
        prev, curr = dummy, self.head
        while curr:
            if curr.val == val:
                prev.next = curr.next
                break
            prev, curr = curr, curr.next
        self.head = dummy.next

    def search(self, val):
        curr = self.head
        while curr:
            if curr.val == val:
                return True
            curr = curr.next
        return False

    def traverse(self):
        curr = self.head
        while curr:
            print(curr.val, end=" -> ")
            curr = curr.next
        print("None")
```

* **Complexities**: insert/delete at head O(1); insert at tail O(n) if no tail pointer; search O(n).
* Can maintain tail pointer to optimize tail insert.

#### Doubly Linked List

* Nodes have `prev` and `next`. Allows O(1) deletion given node reference.

```python
class DNode:
    def __init__(self, val):
        self.val = val
        self.prev = None
        self.next = None

class DoublyLinkedList:
    def __init__(self):
        self.head = None
        self.tail = None

    def insert_at_head(self, val):
        node = DNode(val)
        if not self.head:
            self.head = self.tail = node
        else:
            node.next = self.head
            self.head.prev = node
            self.head = node

    def insert_at_tail(self, val):
        node = DNode(val)
        if not self.tail:
            self.head = self.tail = node
        else:
            self.tail.next = node
            node.prev = self.tail
            self.tail = node

    def delete_node(self, node):
        if not node:
            return
        if node.prev:
            node.prev.next = node.next
        else:
            self.head = node.next
        if node.next:
            node.next.prev = node.prev
        else:
            self.tail = node.prev

    def search(self, val):
        curr = self.head
        while curr:
            if curr.val == val:
                return curr
            curr = curr.next
        return None
```

#### Circular Linked List

* Singly or doubly; last node points to head.
* Useful for round-robin scheduling.

### Stacks

* LIFO. Use list or `collections.deque`.
* Push: `append`, Pop: `pop`.

```python
stack = []
stack.append(1)
stack.append(2)
print(stack.pop())  # 2
```

### Queues

#### Simple Queue

* FIFO. `collections.deque` is efficient: popleft() O(1).

```python
from collections import deque
q = deque()
q.append(1)
q.append(2)
print(q.popleft())  # 1
```

#### Deque (Double-Ended Queue)

* O(1) operations on both ends.

### Hash Table (Hash Map)

* Underlying implementation of dict.
* For custom hash-table: use list of buckets with simple modulo hash for integers or custom hash functions.

```python
class HashMap:
    def __init__(self, size=1000):
        self.size = size
        self.buckets = [[] for _ in range(size)]

    def _hash(self, key):
        return hash(key) % self.size

    def set(self, key, value):
        idx = self._hash(key)
        bucket = self.buckets[idx]
        for i, (k, v) in enumerate(bucket):
            if k == key:
                bucket[i] = (key, value)
                return
        bucket.append((key, value))

    def get(self, key):
        idx = self._hash(key)
        bucket = self.buckets[idx]
        for k, v in bucket:
            if k == key:
                return v
        raise KeyError(key)

    def delete(self, key):
        idx = self._hash(key)
        bucket = self.buckets[idx]
        for i, (k, _) in enumerate(bucket):
            if k == key:
                bucket.pop(i)
                return
        raise KeyError(key)
```

* Real-world: prefer built-in dict for reliability and performance.

### Trees

#### Binary Tree

* Each node: left, right.
* Traversals: preorder, inorder, postorder (recursive or iterative).

```python
class TreeNode:
    def __init__(self, val):
        self.val = val
        self.left = None
        self.right = None

# Traversals
def inorder(root):
    if root:
        yield from inorder(root.left)
        yield root.val
        yield from inorder(root.right)
```

#### Binary Search Tree (BST)

* Left subtree < node < right subtree.
* Operations: insert, search, delete.

```python
class BSTNode:
    def __init__(self, val):
        self.val = val
        self.left = None
        self.right = None

class BST:
    def __init__(self):
        self.root = None

    def insert(self, val):
        def _insert(node, val):
            if not node:
                return BSTNode(val)
            if val < node.val:
                node.left = _insert(node.left, val)
            elif val > node.val:
                node.right = _insert(node.right, val)
            return node
        self.root = _insert(self.root, val)

    def search(self, val):
        node = self.root
        while node:
            if val == node.val:
                return True
            node = node.left if val < node.val else node.right
        return False

    def delete(self, val):
        def _min_value_node(n):
            while n.left:
                n = n.left
            return n
        def _delete(node, val):
            if not node:
                return None
            if val < node.val:
                node.left = _delete(node.left, val)
            elif val > node.val:
                node.right = _delete(node.right, val)
            else:
                if not node.left:
                    return node.right
                if not node.right:
                    return node.left
                temp = _min_value_node(node.right)
                node.val = temp.val
                node.right = _delete(node.right, temp.val)
            return node
        self.root = _delete(self.root, val)
```

* **Complexity**: average O(log n) for balanced; worst-case O(n).
* For balanced BST: consider self-balancing trees (AVL, Red-Black) or use built-in modules or third-party.

#### Heap (Priority Queue)

* Min-heap or max-heap. Python: `heapq` implements min-heap.
* Operations: push/pop O(log n).

```python
import heapq
heap = []
heapq.heappush(heap, 5)
heapq.heappush(heap, 1)
print(heapq.heappop(heap))  # 1
```

* For max-heap: push negative or implement wrapper.

#### Trie (Prefix Tree)

* Efficient retrieval of strings/prefixes.

```python
class TrieNode:
    def __init__(self):
        self.children = {}
        self.is_end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def insert(self, word):
        node = self.root
        for ch in word:
            if ch not in node.children:
                node.children[ch] = TrieNode()
            node = node.children[ch]
        node.is_end = True

    def search(self, word):
        node = self.root
        for ch in word:
            if ch not in node.children:
                return False
            node = node.children[ch]
        return node.is_end

    def starts_with(self, prefix):
        node = self.root
        for ch in prefix:
            if ch not in node.children:
                return False
            node = node.children[ch]
        return True
```

* **Complexity**: Insert/search O(m) where m = length of word.

### Graphs

* Representations: adjacency list (dict mapping node to list of neighbors), adjacency matrix (2D list), edge list.

```python
# Example: adjacency list
graph = {
    'A': ['B', 'C'],
    'B': ['A', 'D'],
    'C': ['A', 'D'],
    'D': ['B', 'C']
}
```

* Use classes to encapsulate graph operations:

```python
class Graph:
    def __init__(self):
        self.adj = {}

    def add_node(self, u):
        if u not in self.adj:
            self.adj[u] = []

    def add_edge(self, u, v, directed=False):
        self.add_node(u)
        self.add_node(v)
        self.adj[u].append(v)
        if not directed:
            self.adj[v].append(u)
```

### Advanced Structures

#### Union-Find (Disjoint Set)

```python
class UnionFind:
    def __init__(self, n):
        self.parent = list(range(n))
        self.rank = [0]*n

    def find(self, x):
        if self.parent[x] != x:
            self.parent[x] = self.find(self.parent[x])
        return self.parent[x]

    def union(self, x, y):
        px, py = self.find(x), self.find(y)
        if px == py:
            return False
        if self.rank[px] < self.rank[py]:
            self.parent[px] = py
        elif self.rank[px] > self.rank[py]:
            self.parent[py] = px
        else:
            self.parent[py] = px
            self.rank[px] += 1
        return True
```

* Use-case: cycle detection, Kruskal’s MST.

#### Segment Tree

* Range query and update.
* Implementation for sum or min. Often recursive or iterative.

```python
class SegmentTree:
    def __init__(self, data, func=min, default=float('inf')):
        self.n = len(data)
        self.size = 1
        while self.size < self.n:
            self.size <<= 1
        self.tree = [default] * (2*self.size)
        self.func = func
        self.default = default
        # Build
        for i in range(self.n):
            self.tree[self.size + i] = data[i]
        for i in range(self.size - 1, 0, -1):
            self.tree[i] = func(self.tree[2*i], self.tree[2*i+1])

    def update(self, idx, value):
        i = idx + self.size
        self.tree[i] = value
        while i > 1:
            i //= 2
            self.tree[i] = self.func(self.tree[2*i], self.tree[2*i+1])

    def query(self, l, r):  # inclusive l, inclusive r
        res = self.default
        l += self.size
        r += self.size
        while l <= r:
            if l % 2 == 1:
                res = self.func(res, self.tree[l])
                l += 1
            if r % 2 == 0:
                res = self.func(res, self.tree[r])
                r -= 1
            l //= 2; r //= 2
        return res
```

#### Fenwick Tree (Binary Indexed Tree)

* For prefix sums/updates.

```python
class FenwickTree:
    def __init__(self, n):
        self.n = n
        self.tree = [0]*(n+1)

    def update(self, idx, delta):  # idx: 0-based
        i = idx+1
        while i <= self.n:
            self.tree[i] += delta
            i += i & -i

    def query(self, idx):  # prefix sum [0..idx]
        res = 0
        i = idx+1
        while i > 0:
            res += self.tree[i]
            i -= i & -i
        return res

    def range_query(self, l, r):
        return self.query(r) - (self.query(l-1) if l>0 else 0)
```

---

## Algorithms

### Searching

#### Linear Search

* Iterate through list, O(n).

```python
def linear_search(arr, target):
    for i, v in enumerate(arr):
        if v == target:
            return i
    return -1
```

#### Binary Search

* Sorted array, divide-and-conquer, O(log n).

```python
def binary_search(arr, target):
    left, right = 0, len(arr)-1
    while left <= right:
        mid = (left + right)//2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    return -1
```

### Sorting

#### Bubble Sort

* Repeatedly swap adjacent out-of-order elements. O(n^2).

```python
def bubble_sort(arr):
    n = len(arr)
    for i in range(n):
        swapped = False
        for j in range(0, n-i-1):
            if arr[j] > arr[j+1]:
                arr[j], arr[j+1] = arr[j+1], arr[j]
                swapped = True
        if not swapped:
            break
```

#### Selection Sort

* Select min from unsorted and swap. O(n^2).

```python
def selection_sort(arr):
    n = len(arr)
    for i in range(n):
        min_idx = i
        for j in range(i+1, n):
            if arr[j] < arr[min_idx]:
                min_idx = j
        arr[i], arr[min_idx] = arr[min_idx], arr[i]
```

#### Insertion Sort

* Build sorted portion by inserting. O(n^2), good for nearly-sorted.

```python
def insertion_sort(arr):
    for i in range(1, len(arr)):
        key = arr[i]
        j = i-1
        while j >= 0 and arr[j] > key:
            arr[j+1] = arr[j]
            j -= 1
        arr[j+1] = key
```

#### Merge Sort

* Divide and conquer, O(n log n), stable.

```python
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    mid = len(arr)//2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])
    return merge(left, right)

def merge(left, right):
    res = []
    i = j = 0
    while i < len(left) and j < len(right):
        if left[i] <= right[j]:
            res.append(left[i]); i += 1
        else:
            res.append(right[j]); j += 1
    res.extend(left[i:]); res.extend(right[j:])
    return res
```

#### Quick Sort

* Divide and conquer, average O(n log n), worst O(n^2).

```python
def quick_sort(arr):
    def _quick(lo, hi):
        if lo < hi:
            p = partition(lo, hi)
            _quick(lo, p-1)
            _quick(p+1, hi)
    def partition(lo, hi):
        pivot = arr[hi]
        i = lo
        for j in range(lo, hi):
            if arr[j] < pivot:
                arr[i], arr[j] = arr[j], arr[i]
                i += 1
        arr[i], arr[hi] = arr[hi], arr[i]
        return i
    _quick(0, len(arr)-1)
```

#### Heap Sort

* Build heap, pop elements. O(n log n).

```python
import heapq
def heap_sort(arr):
    heapq.heapify(arr)
    res = []
    while arr:
        res.append(heapq.heappop(arr))
    return res
```

### Recursion

* Functions calling themselves. Understand base case and recursion tree. Watch for stack limits.

Example: factorial, Fibonacci (with memoization recommended).

```python
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n-1)
```

### Dynamic Programming

Divide problems into overlapping subproblems.

#### Memoization (Top-Down)

```python
from functools import lru_cache
@lru_cache(None)
def fib(n):
    if n < 2:
        return n
    return fib(n-1) + fib(n-2)
```

#### Tabulation (Bottom-Up)

```python
def fib(n):
    if n < 2:
        return n
    dp = [0]*(n+1)
    dp[1] = 1
    for i in range(2, n+1):
        dp[i] = dp[i-1] + dp[i-2]
    return dp[n]
```

### Greedy Algorithms

* Make locally optimal choices. Works when greedy choice property and optimal substructure hold.

Example: Activity selection, coin change (for canonical coin systems), fractional knapsack.

### Backtracking

* Explore possibilities via recursion, undo choices. Useful for permutations, combinations, N-Queens, Sudoku.

```python
def backtrack(path, choices):
    if goal_condition:
        result.append(path.copy())
        return
    for choice in choices:
        # make choice
        path.append(choice)
        # modify choices if needed
        backtrack(path, new_choices)
        # undo
        path.pop()
```

### Graph Algorithms

#### Traversal: BFS and DFS

```python
from collections import deque

def bfs(graph, start):
    visited = set([start])
    q = deque([start])
    order = []
    while q:
        u = q.popleft()
        order.append(u)
        for v in graph.get(u, []):
            if v not in visited:
                visited.add(v)
                q.append(v)
    return order

def dfs(graph, start):
    visited = set()
    order = []
    def _dfs(u):
        visited.add(u)
        order.append(u)
        for v in graph.get(u, []):
            if v not in visited:
                _dfs(v)
    _dfs(start)
    return order
```

#### Shortest Path

* **Dijkstra**: Non-negative weights. O((V+E) log V).

```python
import heapq
def dijkstra(graph, src):  # graph: dict u->list of (v, weight)
    dist = {u: float('inf') for u in graph}
    dist[src] = 0
    heap = [(0, src)]
    while heap:
        d, u = heapq.heappop(heap)
        if d > dist[u]: continue
        for v, w in graph[u]:
            nd = d + w
            if nd < dist[v]:
                dist[v] = nd
                heapq.heappush(heap, (nd, v))
    return dist
```

* **Bellman-Ford**: Handles negative weights, O(VE).

#### Minimum Spanning Tree

* **Kruskal**: Sort edges, use Union-Find. O(E log E).
* **Prim**: Use min-heap starting from any node. O(E log V).

#### Topological Sort

* For DAGs. Kahn’s algorithm (BFS) or DFS-based.

```python
from collections import deque
def topo_sort(graph):
    indegree = {u: 0 for u in graph}
    for u in graph:
        for v in graph[u]:
            indegree[v] = indegree.get(v, 0) + 1
    q = deque([u for u in indegree if indegree[u]==0])
    order = []
    while q:
        u = q.popleft()
        order.append(u)
        for v in graph.get(u, []):
            indegree[v] -= 1
            if indegree[v] == 0:
                q.append(v)
    if len(order) != len(indegree):
        raise ValueError("Graph has a cycle")
    return order
```

### Divide and Conquer

* Break problem into subproblems, solve and combine.
* Examples: merge sort, quick sort, binary search, closest pair.

---

## Problem-Solving Patterns

### Sliding Window

* Fixed or variable window on array/string for sums, max/min substrings, etc.

### Two Pointers

* For sorted arrays or linked lists: find pairs, remove duplicates, etc.

### Fast and Slow Pointers

* Detect cycles, find middle of linked list.

### Binary Search on Answer

* For monotonic functions: find minimal feasible value.

### Bit Manipulation

* Use bit operations for subsets, parity, swapping, optimizations.

### Windowed Data Structures

* Maintain deque or multiset for max/min in sliding window.

---

## Testing and Debugging

* Write unit tests using `unittest` or `pytest`.
* Use assertions to check invariants in data structures.
* For recursive algorithms, print call stack or use small examples.
* Profile performance using time measurements for large inputs.

---

## Tips for Interviews

* Clarify problem requirements and constraints.
* Discuss complexity before coding.
* Start with simple brute-force, then optimize.
* Write clean code with descriptive names.
* Handle edge cases (empty inputs, single element).
* Use built-ins when appropriate, but be ready to implement manually if asked.
* Practice common patterns and problems on platforms like LeetCode, HackerRank.

---

## Conclusion

This reference provides core data structures, algorithms, and patterns in Python, complete with example implementations and complexity notes. Use this as a study guide, implement variations, and apply to real-world problems or interview prep. Keep practicing and refining implementations for performance and clarity.
